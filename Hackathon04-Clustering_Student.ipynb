{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<font size=6  color=#003366> <b>[LEPL1109] - STATISTICS AND DATA SCIENCES</b> <br><br> \n", "<b>Hackathon 04 - Clustering: What is it all about?</b> </font> <br><br><br>\n", "\n", "<font size=5  color=#003366>\n", "Prof. D. Hainaut<br>\n", "Prof. L. Jacques<br>\n", "\n", "<br><br>\n", "Anne-Sophie Collin   (anne-sophie.collin@uclouvain.be)<br>\n", "Guillaume Van Dessel (guillaume.vandessel@uclouvain.be)<br>\n", "Antoine Legat (antoine.legat@uclouvain.be)<br>\n", "J\u00e9rome Eertmans (jerome.eertmans@uclouvain.be)<br>\n", "Maxime Zanella (maxime.zanella@uclouvain.be)<br>\n", "Florine Thiry (florine.thiry@uclouvain.be)<br>\n", "Baptiste Standaert (baptiste.standaert@uclouvain.be)<br>\n", "<div style=\"text-align: right\"> Version 2022</div>\n", "\n", "<br><br>\n", "</font>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-danger\">\n", "    This notebook is meant to be <b>displayed on a web browser</b>. If you are using Visual Studio, or anything else, things might not be displayed as expected. \n", "</div>\n", "\n", "<font size=5 color=#009999> <b>GUIDELINES & DELIVERABLES</b> </font> <br>\n", "-  Copying code or answers from other groups (or from the internet) is strictly forbidden. <b>Each source of inspiration (stack overflow, git, other groups, ...) must be clearly indicated!</b> (We can detect plagiarism)\n", "-  This <b>notebook</b> (\".ipynb\" file) and the <b>report</b> (\".pdf\" file) must be delivered on <b>Moodle</b> before **Friday, the 23rd of December 23h59**.\n", "<!--- - We provide you with a <b>latex template of the report</b> which can be found on Moodle. It is <b>mandatory to answer the questions in the spaces provided</b>. Gradescope will not work if answers are outside these spaces.--->\n", "- The <b>grading</b> will be determined based on the <b>quality</b> of your <b>results</b> and your <b>report</b>. <b>Not</b> on the quality of your <b>code</b>. This <b>notebook</b> serves as a basis for the deliverables but it will <b>not</b> be <b>evaluated</b>.<br><br>\n", "\n", "<img src=\"Imgs/pokemon_go.png\" width = \"600\">\n", "\n", "<div class=\"alert alert-danger\">\n", "<b>[DELIVERABLE] Summary</b>  <br>\n", "After the reading of this document (and playing with the code!), we expect you to provide us with:\n", "<ol>\n", "   <li> a PDF report written with the LaTeX template available on Moodle. It is <b>mandatory to keep the template as provided</b>;\n", "   <li> and this Jupyter Notebook (it will not be read, just checked for plagiarism);\n", "</ol>\n", "\n", "before <b>Friday, the 23rd of December 23h59</b> on Moodle.\n", "\n", "<b>Please</b>, export your image in <b>PDF</b> format, not JPEG or PNG, beforing including them in your report.<br> Exporting images in PDF is as simple as using the appropriate filename extension (see example below)!\n", "</div>\n", "\n", "```python\n", "# with matplotlib\n", "plt.savefig(\"my_image.pdf\")\n", "\n", "# with plotly\n", "fig.write_image(\"my_image.pdf\")\n", "```\n", "\n", "<font size=5 color=#009999> <b>CONTEXT & NOTEBOOK STRUCTURE</b> </font> <br>\n", "    \n", "The objective of this hackathon is threefold:\n", "1. extract meaningful information from a dataset;\n", "2. observe relationship(s) (if any) between features and eventual underlying groups (clusters);\n", "3. and develop an unsupervised clustering tool and exploit the associated data.\n", "\n", "To this end, you will use a synthetic dataset (available on Moodle) inspired from a real dataset that can be found on [Kaggle](https://www.kaggle.com/datasets/kveykva/sf-bay-area-pokemon-go-spawns). Given a couple of features, you should be able to **create Pok\u00e9mon clusters based on spatial-temporal coordinates and other prodived features. Then, you must exploit the content of these different clusters to determine the likeliness of capturing a given Pok\u00e9mon for some provided input requests, such as time and position.**\n", "\n", "<img src=\"Imgs/IllustrationContext.png\" width = \"800\">\n", "\n", "Nowadays, mobile games are gaining more and more attention, sometimes [maybe too much](https://www.ouest-france.fr/leditiondusoir/2021-01-21/le-jeu-pokemon-go-aurait-cause-de-nombreux-accidents-de-la-route-et-coute-des-milliards-94619b3e-8c13-40d4-9d2a-ef3705754fe7). This is especially true for the well known [Pok\u00e9mon GO](https://pokemongolive.com/en/).\n", "\n", "> **For those who do not know:** Pok\u00e9mon GO is a mobile-game in which players have to capture as many Pok\u00e9mon as possible. Pok\u00e9mon are creatures that randomly spawn (i.e., appear) at different positions and times, but some locations are more likely to have Pok\u00e9mon appearing: shopping malls, city centers, parks, and so on. Once a Pok\u00e9mon has spawned (i.e., appeared), the players have to physically go to the same place as the Pok\u00e9mon to hopefully capture it.\n", "\n", "As a casual Pok\u00e9mon GO player and a proficient data scientist, you would like to increase your level by leveraging some data-related techniques. To this end, you found a Pok\u00e9mon GO spawning versus localization dataset that you will use to, hopefully, achieve your goals (see above).\n", "\n", "This notebook is organized into three parts. Each of them assesses one fundamental step to solve your problem:\n", "\n", "* PART 1 - DATA PREPROCESSING\n", "   - 1.1 - Import the data\n", "   - 1.2 - Features preprocessing\n", "    <br><br>\n", "* PART 2 - DATA VISUALIZATION\n", "   - 2.1 - Dataset exploration\n", "   - 2.2 - Spatial features visualization\n", "   - 2.3 - PCA\n", "    <br><br>\n", "* PART 3 - IT'S TIME TO... CLUSTER!\n", "   - 3.1 - Clustering : definition, example and execution\n", "   - 3.2 - Results analysis\n", "\n", "Your answers to the questions in this notebook must be written in your report. We filled this notebook with preliminary (trivial) code. This practice makes possible to run each cell, **up to part 3 (excl.)**, without throwing warnings. <b>Take advantage of this aspect to divide the work between all team members!</b>\n", "\n", "Finally, we strongly advise you to <b>read the whole notebook once</b> before jumping in.<br><br>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# This allows you to show/hide the cells with code\n", "# from https://mljar.com/blog/jupyter-notebook-hide-code/\n", "\n", "# WARNING: this might not work on Visual Studio Code\n", "\n", "from IPython.display import HTML\n", "\n", "HTML(\n", "    \"\"\"<script>\n", "code_show=true; \n", "function code_toggle() {\n", " if (code_show){\n", " $('div.input').hide();\n", " } else {\n", " $('div.input').show();\n", " }\n", " code_show = !code_show\n", "} \n", "</script>\n", "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to show/hide the code.\"></form>\"\"\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b01: INSTALL DEPENDENCIES\n", "Make sure you have everything installed\n", "\"\"\"\n", "\n", "import sys\n", "from IPython.utils import io\n", "\n", "with io.capture_output() as captured:\n", "    !{sys.executable} -m pip install -r requirements.txt\n", "\n", "\n", "if \"ERROR\" in captured.stdout:\n", "    print(captured.stdout)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%javascript\n", "IPython.OutputArea.prototype._should_scroll = function(lines) {\n", "    return false;\n", "}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<br><font size=7 color=#009999> <b>PART 1 - DATA PREPROCESSING</b> </font> <br><br>\n", "\n", "<font size=5 color=#009999> <b>1.1 - IMPORT THE DATA</b> <br>\n", "Pok\u00e9mon dataset \n", "</font> <br> (synthetic dataset inspired from [Pok\u00e9mon Spawns](https://www.kaggle.com/datasets/kveykva/sf-bay-area-pokemon-go-spawns)) <br>\n", "\n", "\n", "In this dataset, you are provided some data related to the Pok\u00e9mon spawns. This includes geolocation, temporal and Pok\u00e9mon data-related information:\n", "\n", "- `type`: refers to different elemental properties associated with each Pok\u00e9mon\n", "- `lat`: spawn latitude\n", "- `lng`: spawn longitude\n", "- `name`: the Pok\u00e9mon species\n", "- `date`: time at which the spawn was recorded\n", "- `num`: index of referencing in the Pok\u00e9dex\n", "- `id`: key for spawn identification\n", "- `appear_duration`: Pok\u00e9mon appearance time (in seconds), i.e., the time after which the Pok\u00e9mon disappears\n", "\n", "Take a look at the next cell to see the exact composition of the dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b02: IMPORT THE DATASET\n", "\"\"\"\n", "from collections import Counter\n", "\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import pandas as pd\n", "import plotly.express as px\n", "import plotly.figure_factory as ff\n", "import toolbox as tb\n", "\n", "from IPython.display import display, Markdown as md\n", "from PIL import Image\n", "from sklearn.cluster import KMeans\n", "from sklearn.decomposition import PCA\n", "from sklearn import datasets, preprocessing\n", "\n", "pd.set_option(\"max_colwidth\", 200)\n", "\n", "path = \"Data/pokemon-go-dataset.csv\"\n", "df = pd.read_csv(path, parse_dates=[\"date\"])\n", "\n", "# Print first rows\n", "display(md(\"# Pokemon spawns\"))\n", "display(df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<br>\n", "<font size=5 color=#009999> <b>1.2 - FEATURES PREPROCESSING</b> <br>\n", "REMOVING UNNECESSARRY INFORMATION, CLEANING DATASET AND CREATING NEW FEATURES\n", "</font> <br> <br>\n", "\n", "When doing data sciences, the datasets you are using were most probably not made for your very application. Instead, they result from the collection of information throughout a certain period of time, and it is the data scientist's job to make a good use of those datasets.\n", "\n", "For this hackathon, your goal is to **determine the Pok\u00e9mon individual (i.e., name) that is the most likely to spawn based on position and time requests**. Therefore, you should be able to determine which features are useful for your application.\n", "\n", "<div class=\"alert alert-warning\">\n", "<b>[Question 1.1] Removing unnecessary features </b>  <br>\n", "Can you already, a priori, detect that some features are useless?\n", "<ol>\n", "   <li> if yes, list those (useless) features and explain your choice;\n", "   <li> if not, then explain why it is better to wait.\n", "</ol>\n", "    Generally speaking, is it a good idea to remove a feature based on <i>a priori</i> knowledge, or doesn't it alter the final outcome?\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b03: REMOVING UNNECESSARY FEATURES\n", "If you answered yes in previous question, please remove those features here.\n", "\n", "WARNING: removing some features might prevent the following cells to run (especially visualization cells).\n", "Please adapt your code accordingly.\n", "\"\"\"\n", "\n", "#########################################################################################################\n", "# Start : Student version\n", "#########################################################################################################\n", "\n", "features = [\n", "    # Fill the list here or leave empty\n", "]\n", "\n", "#########################################################################################################\n", "# End : Student version\n", "#########################################################################################################\n", "\n", "\n", "df.drop(labels=features, axis=1, inplace=True, errors=\"ignore\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-info\">\n", "<b>[Remark 1.1]</b><br>\n", "In most real-cases, the datasets you are going to work with will contain artifacts, such as typos or missing data, that you may want to remove before feeding the data into any algorithm. Here, Pandas treats missing data as \u00b4NaN\u00b4s (refering to Not a Number, even though it is used for every missing object, no only numbers).\n", "</div>\n", "\n", "Can you find a way to inspect your dataset and see if there are some missing data?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b04: INFORMATION ABOUT TYPES AND NANs\n", "Use one (or more) of Pandas' builtin functions to get information about data types\n", "and the number of missing (NaN) values for each feature.\n", "\"\"\";\n", "\n", "#########################################################################################################\n", "# Start : Student version\n", "#########################################################################################################\n", "\n", "\n", "#########################################################################################################\n", "# End : Student version\n", "#########################################################################################################\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-info\">\n", "<b>[Remark 1.2] Each problem has its own solution</b> <br>\n", "There exists numerous ways to deal with missing information and we will discuss the two main approaches:\n", "<ol>\n", "   <li> you remove rows or columns that contain missing data;\n", "   <li> or you replace NaNs with another value. The latter can be a fixed value or computed to be the mean of all non-NaNs values. The topic of replacing missing data, also call imputation of missing values, is very broad and complex, and there is no global solution that applies everywhere. Maybe you can find one that works well here?\n", "</ol>\n", "    \n", "You **should** read more about how to imput missing value [here](https://scikit-learn.org/stable/modules/impute.html).\n", "</div> \n", "\n", "<div class=\"alert alert-warning\">\n", "<b>[Question 1.2] Handling missing data </b>  <br>\n", "Given the dataset and the amount / type of missing information, what strategy do you propose to follow regarding missing data (NaNs) ? <br> You can choose one or many of the following:\n", "<ol>\n", "   <li> drop features (column) with missing information; \n", "   <li> drop samples (row) with missing information;\n", "   <li> replace missing information with interpolation / extrapolation / simple substitution / ...\n", "</ol>\n", "Justify briefly your choice.\n", "</div> "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b05: HANDLING NANs\n", "Apply your handling NaN strategy by filling the gaps according to what you have decided to do.\n", "Do not reinvent the wheel: a good data-scientist is a lazy d-s.\n", "Feel free to modify the code.\n", "\"\"\"\n", "\n", "#########################################################################################################\n", "# Start : Student version\n", "#########################################################################################################\n", "\n", "# This is an example of how you can drop columns or rows containing NaNs,\n", "# feel free to modify the code if your prefer.\n", "\n", "drop_rows = [\n", "    # Fill the list here or leave empty\n", "]\n", "# set `drop_rows = None` to check for all columns\n", "# set `drop_rows = []` to check for no column\n", "\n", "drop_cols = [\n", "    # Fill the list here or leave empty\n", "]\n", "# set `drop_cols = df.columns` to select all columns\n", "# set `drop_rows = []` to select no column\n", "\n", "# Fill special substitution here (e.g., every NaN becomes an empty string \"\")\n", "\n", "#########################################################################################################\n", "# End : Student version\n", "#########################################################################################################\n", "\n", "\n", "df.dropna(\n", "    axis=0, subset=drop_rows, inplace=True\n", ")  # Drop rows with NaN in any of the mentionned columns\n", "\n", "for col in drop_cols:\n", "    # For each mentionned column, drop it if it contains `any` NaN\n", "    if df[col].isnull().values.any():\n", "        df.drop(col, axis=1, inplace=True)\n", "\n", "df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-info\">\n", "<b>[Remark 1.3] New features extraction</b> <br>\n", "In the present case, some features in the dataset still need to be reworked in order to provide meaningful information. For example, working with datetimes might not be easy.\n", "</div>\n", "\n", "You may want to somehow incorporate the information about date and time into the dataset in a more **intelligent** manner than it was before. Again, there can be multiple solutions, and we will propose you a very simple one.\n", "\n", "For example, you may be interested only in the day of a spawn, not the exact second. Other features can also be created, and it is up to you to find the best ones! In the previous hackathon, e.g., you learned about a specific mapping for encoding periodic data.\n", "\n", "The following code will show you how to get the day. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b06: CREATING FEATURES\n", "Here is how you can extract the date from de datetime feature.\n", "You can create here your own ones!\n", "\"\"\"\n", "\n", "# E.g., get day information (as a number)\n", "df[\"day\"] = df.date.dt.day\n", "\n", "#########################################################################################################\n", "# Start : Student version\n", "#########################################################################################################\n", "\n", "# Add the features you want\n", "\n", "#########################################################################################################\n", "# End : Student version\n", "#########################################################################################################\n", "\n", "\n", "display(\n", "    md(\n", "        \"#### Here is how your dataset looks like now that you have added your new feature(s):\"\n", "    )\n", ")\n", "df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-warning\">\n", "<b>[Question 1.3] New features </b>  <br>\n", "What features have you added? If a particular manipulation has been applied, please explain.\n", "</div> \n", "\n", "<br><font size=7 color=#009999> <b>PART 2 - DATA VISUALIZATION</b> </font> <br><br>\n", "\n", "Now that we are done with handling and preprocessing the data, we will further dig into the dataset to see what it actually contains.\n", "\n", "<br><font size=5 color=#009999> <b>PART 2.1 - DATASET EXPLORATION</b> </font> <br><br>\n", "\n", "Bringing an **understanding of data** to data scientists **is crucial**. Visualizing the distributions of each feature generally provides good insight into how that data is distributed. \n", "\n", "There exists plenty of ways for visualizing big amount of data. Indeed, data visualization is a whole branch of data science which aims to make accessible the ways to view and understand data.\n", "\n", "Hopefully, there are many amazing tools that allow you to plot those data incredibly. We provide you with three simple examples of those."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b07: SPAWNS PER DATETIME\n", "\"\"\"\n", "\n", "fig = df.sample(frac=0.01, random_state=1234)[\"date\"].hist(\n", "    backend=\"plotly\", title=\"Count of Pok\u00e9mon spawns per datetime\"\n", ")\n", "\n", "fig.update_layout(\n", "    xaxis_title=\"Datetime\",\n", "    yaxis_title=\"Count\",\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b08: WORDCLOUD OF POKEMON\n", "\"\"\"\n", "\n", "counter = Counter(df.name)\n", "tb.word_cloud(\n", "    counter, title=\"WordCloud of Pok\u00e9mon names - Bigger means more occurences\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b09: APPEARANCE DURATION\n", "\"\"\"\n", "\n", "fig = df[\"appear_duration\"].hist(\n", "    backend=\"plotly\", title=\"Count of Pok\u00e9mon appearance durations\"\n", ")\n", "\n", "fig.update_layout(\n", "    xaxis_title=\"Appearance duration\",\n", "    yaxis_title=\"Count\",\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-warning\">\n", "<b>[Question 2.1] Features visualization </b> <br>\n", "    \n", "Based on what you have seen above and what you will try, you can already get an idea of which features seem to contain discriminative information, i.e., which features are likely to be more important for the clustering than others. Please, explain.\n", "    \n", "Justify which features you think would be interesting or not to keep in order to reach your goals.\n", "Feel free to try and add your own data visualization to highlight or not their importance.\n", "</div>\n", "\n", "<div class=\"alert alert-info\">\n", "<b>[Remark 2.1] Features visualization</b> <br>\n", "As mentioned before, there are many ways to visualize data. \n", "    \n", "Do not hesitate to support your arguments with original plots to illustrate your point. Feel also free to illustrate the potential new features you have introduced earlier.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b010: YOUR OWN VISUALIZATIONS\n", "Feel free to create more plots here (or elsewhere).\n", "\"\"\";"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-info\">\n", "<b>[Remark 2.2] Feature pruning</b> <br>\n", "Based on your previous visualizations and choices, reuse the code at the top of the notebook to drop features that are unnecessary. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b011: REMOVING UNNECESSARY FEATURES\n", "You can use this cell to drop other features you consider unnecessary.\n", "\"\"\";\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<br><font size=5 color=#009999> <b>PART 2.2 - SPATIAL FEATURES VISUALIZATION</b> </font> <br><br>\n", "\n", "As our dataset contains spatial coordinates for each Pok\u00e9mon spawn, it is therefore imperative to correctly visualize the data concerning the localization of Pok\u00e9mon.\n", "\n", "Since this location information are based on real coordinates, we will be able to visualize this data on real maps. We therefore provide a set of tools that will allow you to observe the distribution of this data on a world map and, more precisely, in San Fransisco.\n", "\n", "Further, <a href=\"https://www.reddit.com/r/pokemongodev/comments/4v3tkt/spawnpoint_classification/\">an analysis carried out by redditors on the spawn mechanics</a> of different Pok\u00e9mon has revealed that, depending on the ground assignment, some pok\u00e9mon are more likely to spawn. Indeed, it has been observed that the game tends to <b>favor the spawn of certain Pok\u00e9mon of the same type according to the land cover</b> (e.g., park, forest, industrial site, water point, ...).\n", "\n", "Luckily, the tool provided also makes it possible to visualize the land covers distribution. This distribution was carried out on the basis of public data provided by [CORINE Land Cover project](https://land.copernicus.eu/pan-european/corine-land-cover). We will distinguish 8 categories of soil distribution, namely: `forests`, `shrubland`, `herbaceous vegetation`, `herbaceous wetland`, `moss & lichen`, `bar/spare vegetation`, `cropland`, `built-up`, `snow & ice` and `permanent water bodies`.\n", "\n", "<div class=\"alert alert-info\">\n", "<b>[Remark 2.3] Land cover</b> <br>\n", "Even if it is possible to obtain those land covers, <b>you are not allowed to use</b> this information for your clustering, it would be too complicated. \n", "    \n", "However, the fact that Pok\u00e9mon spawns <b>by type</b> according to the land cover can still be useful for your classification problem... It is up to you to understand how.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b012: SAN FRANSISCO VISUALIZATION\n", "\n", "Read the code carefully and try to understand it.\n", "\"\"\"\n", "sub_df = df.sample(frac=0.001, random_state=1234)\n", "\n", "san_fransisco = {\n", "    \"lat\": 37.573972,\n", "    \"lon\": -122.431297,\n", "}\n", "\n", "fig = px.scatter_mapbox(\n", "    sub_df,\n", "    lat=sub_df.lat,\n", "    lon=sub_df.lng,\n", "    color=\"name\",  # which column to use to set the color of markers\n", "    hover_name=\"name\",  # column added to hover information\n", "    center=san_fransisco,\n", "    mapbox_style=\"open-street-map\",\n", "    color_discrete_sequence=px.colors.qualitative.Dark24,\n", ")\n", "\n", "fig.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b013: LAND USE VISUALIZATION\n", "\n", "Read the code carefully and try to understand it.\n", "\"\"\"\n", "\n", "# Land use data from:\n", "# Global land cover - https://lcviewer.vito.be/2019\n", "\n", "coordinates = [\n", "    [-123.3085317, 38.5763889],\n", "    [-121.4027778, 38.5763889],\n", "    [-121.4027778, 37.1230159],\n", "    [-123.3085317, 37.1230159],\n", "]\n", "\n", "image = Image.open(\"Data/landuse_sf.tif\")\n", "\n", "fig = px.scatter_mapbox(\n", "    sub_df,\n", "    lat=sub_df.lat,\n", "    lon=sub_df.lng,\n", "    color=\"type\",  # which column to use to set the color of markers\n", "    hover_name=\"type\",  # column added to hover information\n", "    center=san_fransisco,\n", "    mapbox_style=\"white-bg\",\n", "    color_discrete_sequence=px.colors.qualitative.Dark24,\n", ")\n", "\n", "fig.update_layout(\n", "    mapbox_style=\"white-bg\",\n", "    mapbox_layers=[\n", "        {\n", "            \"below\": \"traces\",\n", "            \"sourcetype\": \"image\",\n", "            \"source\": image,\n", "            \"coordinates\": coordinates,\n", "            \"opacity\": 0.5,\n", "        }\n", "    ],\n", ")\n", "\n", "fig.add_layout_image(\n", "    source=\"Imgs/land_use_legend.png\",\n", "    xref=\"paper\",\n", "    yref=\"paper\",\n", "    x=0,\n", "    y=0.0,\n", "    xanchor=\"left\",\n", "    yanchor=\"bottom\",\n", "    sizex=1,\n", "    sizey=1,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-warning\">\n", "<b>[Question 2.2] Spatial features visualization </b> <br>\n", "\n", "Based on the maps above, what can you infer about the spawn locations of the Pok\u00e9mon? Is there a link between their types and the land cover? If yes, explain. \n", "</div>\n", "  \n", "<div class=\"alert alert-warning\">\n", "<b>[Question 2.3] Spatial clustering </b> <br>\n", "Based on the maps above, i.e., <i>on the spatial features only</i>, how do you think a clustering will perform according to the number of clusters? In other words, what do you think will happen with 1, 15 (= number of types), 100+ clusters?\n", "</div>\n", "\n", "<br>\n", "<font size=5 color=#009999> <b>2.3 - PCA</b> <br>\n", "REDUCE THE DIMENSIONALITY OF THE DATA IN ORDER TO OBSERVE IT\n", "</font> <br> <br>\n", "\n", "<!---\n", "The high dimensionality of the dataset (number of columns) makes data visualization hard. In order to gain some (partial) information about our data distribution,\n", "--->\n", "PCA is often considered as the simplest and most fundamental technique used in dimensionality reduction. Remember that PCA is essentially the rotation of coordinate axes, chosen such that each successful axis captures or preserves as much variance as possible. If the algorithm returns a new system coordinates of the same dimension as the input, we can keep only the axis corresponding to the 3 largest singular values and project data on this coordinates system to perform the visualization.\n", "\n", "\n", "![PCAUrl](https://miro.medium.com/max/400/1*ZXhPoYQIn-Y8mxoUpz5Ayw.gif \"PCA\")\n", "\n", "Although PCA allows to reduce dimensionality for a visualization purpose, it can also give information about the *importance of our features*. Indeed, some of them contribute more to the choice of the principal components and are therefore those that best explain the dataset.\n", "\n", "To vizualize the importance of features, we can extract the [PCA loadings](https://scentellegher.github.io/machine-learning/2020/01/27/pca-loadings-sklearn.html). These are indicators of the correlation between components and original features. The value of loadings is contained between -1 and 1. The more the value goes toward those boundaries, the more the feature influences the choice of component.  \n", "We propose to perform a 2-dimensional PCA and then to add the loadings in vector form to the figure to obtain what is called a *biplot* (see example on the iris dataset below). \n", "\n", ">**NOTE**: even if the figure below resembles the previous one, the former displays the directions of the 3 principal components while this one shows the importance of each feature."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b014: LOADINGS EXAMPLE ON A SEPARATE TOY DATASET\n", "Plot of the reduced iris dataset to 2 dimensions with the loadings (i.e. vectors that represent the importance of each feature)\n", "\n", "There is nothing to do here ;-)\n", "\"\"\"\n", "iris = datasets.load_iris()\n", "\n", "names = iris.target_names\n", "\n", "X = preprocessing.scale(iris.data)\n", "y = [names[i] for i in iris.target]\n", "\n", "pca = PCA(n_components=2)\n", "X_data2 = pca.fit_transform(X)\n", "\n", "tb.biplot_visualization(pca, X_data2, y, columns=iris.feature_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Knowing that you will have to predict <b>Pok\u00e9mon names</b> from spatial and temporal coordinates, you first need to split your dataset into a feature vector (`X`) and a target vector (`y`), then to reduce dimensionality of the feature space to compute and vizualize the loadings.\n", "\n", "\n", "<div class=\"alert alert-warning\">\n", "<b>[Question 2.4] Feature importance visualization </b>  <br>\n", "    Use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA\">PCA</a> function from scikit learn to reduce your data to 2 dimensions. Then, use the <samp>biplot_visualization()</samp> function provided in the toolbox (<i>toolbox.py</i>) to vizualize the biplot graph.\n", "\n", "Do all features have the same importance? If no, which features are less important, and why?\n", "You can use all other graphs from the visualization part to justify your answer.\n", "</div>\n", "\n", "<div class=\"alert alert-info\">\n", "<b>[Remark 2.4] </b> <br>\n", "Feel free to try this vizualization with numerical features you already dropped (if any) to support your previous choices.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b015: FEATURE IMPORTANCE VISUALIZATION\n", "\n", "IMPORTANT: only keep numerical features in X (no NaNs). If needed, you can use a mapping function,e.g., from string\n", "to numerical values. Dates are not numbers!\n", "\"\"\"\n", "\n", "# First, we recommend using a fraction of the datatest\n", "# and you should remove this (of set frac=1) when you\n", "# feel confident about your model\n", "\n", "sub_df = df.sample(frac=0.001, random_state=1234)\n", "\n", "output = \"name\"  # or: output = \"type\" (see remark 3.3)\n", "\n", "# NOTE: if you chose 'type', then you must discard 'name', and vice-versa...\n", "#       ... since predicting Pok\u00e9mon types from names is trivial ;-)\n", "\n", "#########################################################################################################\n", "# Start : Student version\n", "#########################################################################################################\n", "\n", "y = ...  # target output\n", "X = ...  # input DataFrame, without output column(s)\n", "columns = ...  # a vector of input feature names, usually `X.columns`\n", "\n", "# Standardize the data\n", "X = ...  # do some standardization\n", "\n", "# Apply a 2-dimensional PCA\n", "pca = ...\n", "X_data2 = ...\n", "\n", "#########################################################################################################\n", "# End : Student version\n", "#########################################################################################################\n", "\n", "\n", "if all(not isinstance(x, type(...)) for x in [pca, X_data2, columns]):\n", "    fig = tb.biplot_visualization(pca, X_data2, y, columns=columns)\n", "    fig.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-info\">\n", "<b>[Remark 2.5] Feature pruning</b> <br>\n", "Based on your previous conclusions you may want to prune other features. If so, reuse the code at the top of the notebook to drop those features. \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b016: REMOVING UNNECESSARY FEATURES\n", "You can use this cell to drop other features you consider unnecessary.\n", "\"\"\";\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<br> <br>\n", "<font size=7 color=#009999> <b>PART 3 - IT'S TIME TO... CLUSTER!</b> </font> <br><br>\n", "\n", "<br>\n", "<font size=5 color=#009999> <b>3.1 - Clustering: definition, example and execution</b> <br>\n", "THE ABC OF CLUSTERING\n", "</font> <br> <br>\n", "\n", "Clustering can be defined as the task of *grouping* objects from a set $S$ (here, each row/observation is an object) in such a way that objects assigned to the same group (called cluster) are more **similar** (or less **distant**) with respect to each other (in some sense) than to those assigned to the other groups. Usually, we would like to divide our objects into $K$ groups.\n", "\n", "As such, clustering reduces to finding, among all $K$-partitions possible of $S$, the partition $\\mathcal{P}$ that minimizes some error criterion $f(\\mathcal{P})$. Each object will be assigned a cluster, $C_i$, and each cluster will have its centroid $c_i$ the distance between **any object** in $C_i$ to centroid $c_i$ is **always smaller** that the distance to any other centroid. In other words, each object is assigned to the cluster whose centroid is the closest.\n", "\n", "\n", "A mathematical formulation of the problem could be the following, $$ \\boxed{\\min_{(C_1,\\dots,C_K) \\,\\in\\, \\mathcal{P}}\\,f(C_1,\\dots,C_K) = \\sum_{i = 1}^{K}\\,\\sum_{x \\in C_i}\\,\\Delta(x,c_i)}$$\n", "\n", "where $\\Delta(x,c_i)$ denotes the distance between object $x$ and centroid $c_i$.\n", "\n", "<br>\n", "<font size=5 color=#009999>\n", "EXAMPLE OF SEPARATING OBJECTS INTO 10 CLUSTERS\n", "</font> <br> <br>\n", "\n", "**First**, let us imagine the following 2D dataset.\n", "\n", "<img src=\"Imgs/10-partitions-data.svg\" width = \"250\">\n", "\n", "**Then**, a 10-partition is defined by the position of the centroids, one for each cluster. Below, you can observe four examples of (random) centroids localizations (stars).\n", "\n", "<img src=\"Imgs/10-partitions-chose-centroids.svg\" width = \"1000\">\n", "\n", "**Next**, the regions are colored based on their closest centroid. Here, we take the distance to be the Euclidean distance.\n", "\n", "<img src=\"Imgs/10-partitions-centroids.svg\" width = \"1000\">\n", "\n", "**Finally**, data points (objects) are colored based in the region they are in.\n", "\n", "<img src=\"Imgs/10-partitions-clusters.svg\" width = \"1000\">\n", "\n", "\n", "As a reminder, for this hackathon, your goal is to determine the Pok\u00e9mon (i.e., `name` column, i.e., a `str`) that are the most likely to spawn based on position and time requests. Therefore, for each element of the `X_test`, you will have to provide `n = 100` possible Pok\u00e9mon that could have spawn at that particular location and time.\n", "\n", "To do so, you need first to <b>cluster your data</b>, then identify <b>the closest cluster to each element in your test set</b>, and finally **choose from the closest cluster** `n` **Pok\u00e9mon** `name`**s** that could have spawned there (or nearby).\n", "\n", "<div class=\"alert alert-info\">\n", "<b>[Remark 3.1] </b> <br>\n", "In order to evaluate yourself, we have provided in <i>toolbox.py</i> the function <samp>get_ground_truth()</samp> that we will use to create the ground truth vector and return the metric function. Therefore, feel free to create your own validation vector and try your algorithms on it to compare the impact of your design choices.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b017: GROUND TRUTH\n", "Create your own ground truth vector (i.e. split your dataset in train and validation sets)\n", "Reuse X (standardized) and y from previous cell to create those sets.\n", "\n", "IMPORTANT: you may want to change between output = \"name\" and output = \"type\" (see remark 3.3)\n", "\"\"\"\n", "\n", "train_indices, val_indices = tb.train_val_indices(X.shape[0], val_frac=0.1, seed=1234)\n", "\n", "X_train, X_val = X[train_indices, :], X[val_indices, :]\n", "y_train, y_val = y.values[train_indices], y.values[val_indices]\n", "\n", "n = 100\n", "\n", "# Return n closest Pok\u00e9mon (and types) for each entry in X_val\n", "# WARNING: make sure X was generated from sub_df, otherwise update the code\n", "y_true = tb.get_ground_truth(sub_df, sub_df.iloc[val_indices, :], n=n)\n", "\n", "y_true[output]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-warning\">\n", "<b>[Question 3.1] Number of clusters </b>  <br>\n", "    Accounting for all features (i.e., spatial <b>and</b> temporal coordinates), what do you think is the ideal number of clusters? What will happen if too many or even too few clusters are chosen?\n", "</div>\n", "\n", "Now that your dataset is divided into a train and a validation set, use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">KMeans</a> algorithm from `scikit-learn` to apply the clustering on your dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b018: KMEANS CLUSTERING\n", "Use KMeans to group your data into k clusters\n", "\"\"\"\n", "\n", "k = 15  # initial value, you can replace it by whatever you prefer\n", "\n", "#########################################################################################################\n", "# Start : Student version\n", "#########################################################################################################\n", "\n", "kmeans = ...\n", "cluster_y_train = ...  # predict the cluster index for each sample of X_train\n", "\n", "#########################################################################################################\n", "# End : Student version\n", "#########################################################################################################\n", "\n", "\n", "# We create a DataFrame from train data, that will contain the cluster of each entry\n", "X_train_df = pd.DataFrame(data=X_train, columns=columns)\n", "X_train_df[\"cluster\"] = cluster_y_train\n", "X_train_df[output] = y_train\n", "X_train_df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["An easy way of choosing `n` Pok\u00e9mon among a cluster is to look at the one that spawned the most and provide it `n` times as a possible Pok\u00e9mon for the entry you have. This is what `top_output_per_cluster` does, i.e., it returns the output that is the most represented for each cluster.\n", "\n", "> **NOTE:** this solution is a slightly simple and naive approach to the problem that only takes into account the most represented Pok\u00e9mon in the cluster. You are invited to implement, if you wish, your own sampling functions within the cluster. You can, for example, provide a more customized result by taking into account the distribution of Pok\u00e9mon within the cluster itself.<br>\n", "Feel free to try below your own sampling implementations.\n", "\n", "<div class=\"alert alert-warning\">\n", "<b>[Question 3.2] Cluster composition </b>  <br>\n", "Do you think the naive approach will give the best results? Justify briefly.\n", "\n", "What do you think would be the best way to estimate the Pok\u00e9mon you encounter? Explain.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b019: PREDICTING POK\u00c9MON\n", "Predict the n possible Pok\u00e9mon you saw for each elements of your validation set\n", "\n", "You are encourage to vary:\n", "- the number of clusters\n", "- the output type (\"name\" or \"type\") - (see remark 3.3)\n", "- the way Pok\u00e9mon are predicted inside the cluster\n", "\"\"\"\n", "\n", "top_output_per_cluster = X_train_df.groupby([\"cluster\"]).apply(\n", "    lambda df: df[output].value_counts().idxmax()\n", ")\n", "\n", "\n", "def predict_pokemon_using_most_present(X_test, n, top_output_per_cluster, kmeans):\n", "    \"\"\"\n", "    Predicts n Pok\u00e9mon encountered, for each entry in X_test (can be validation set),\n", "    using the most represented Pok\u00e9mon in each cluster, and repeating it n times.\n", "    \"\"\"\n", "    cluster_y_test = kmeans.predict(X_test)\n", "    y_pred = top_output_per_cluster[cluster_y_test]\n", "    y_pred = y_pred.values.reshape(\n", "        -1, 1\n", "    )  # We transform a Pandas DataFrame into a column array\n", "    return np.repeat(y_pred, n, axis=1)\n", "\n", "\n", "#########################################################################################################\n", "# Start : Student version\n", "#########################################################################################################\n", "\n", "# add more tests here\n", "\n", "#########################################################################################################\n", "# End : Student version\n", "#########################################################################################################\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<br>\n", "<font size=5 color=#009999> <b>3.2 - RESULTS ANALYSIS </b> <br>\n", "OBSERVE AND COMPARE\n", "</font> <br> <br>\n", "\n", "In this section, we adress the difficult task of evaluating the performance of the clustering algorithm.\n", "\n", "\n", "<br>\n", "<b>1. Use a metric function.</b> One way to assess the quality of your data partitionning is to use a metric that compares the predicted vector with the true one according to a well chosen function. For this hackathon, you can find in <i>toolbox.py</i>, the function <samp>accuracy_metric()</samp> that contains the metric we will use to evaluate your solution. The value of the metric lies between 0 and 1, 0 being the worse case and 1 the best.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b020: METRIC\n", "\n", "Compute the metric on the validation set.\n", "Make sure to select the correct output, i.e., `output=\"name\"` or `output=\"type\".\n", "\"\"\"\n", "\n", "#########################################################################################################\n", "# Start : Student version\n", "#########################################################################################################\n", "\n", "y_pred = predict_pokemon_using_most_present(X_val, n, top_output_per_cluster, kmeans)\n", "\n", "accuracy = tb.accuracy_metric(y_pred, y_true[output])\n", "\n", "print(\n", "    \"Prediction on\",\n", "    output,\n", "    \"resulted in an average accuracy of\",\n", "    np.mean(accuracy),\n", "    f\"(std.: {np.std(accuracy)})\",\n", ")\n", "\n", "#########################################################################################################\n", "# End : Student version\n", "#########################################################################################################\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<b>2. Compare different solutions.</b> An import thing to do while looking for an optimal solution is to compare the impact of your changes. Since you have access to the metric function, you should be able to assess the evolution of the metric value according to your choice of hyperparameters, methods, etc.  \n", "> **NOTE:**  graphs are great tools for comparing solutions."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b021: COMPARE METHODS\n", "\"\"\"\n", "#########################################################################################################\n", "# Start : Student version\n", "#########################################################################################################\n", "\n", "# Feel free to use this cell to implement your way of comparing your methods\n", "\n", "#########################################################################################################\n", "# End : Student version\n", "#########################################################################################################\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**3. Evaluate your solution on unknown data:** Now that you have made all the choices regarding your clustering solution and your model is ready, you need to evaluate it on new data (i.e. not used during the training step). We provide you with a test set containing 100 new inputs with their targets, the 100 closest Pok\u00e9mon (`name` or `type`). Apply your method to this data and use the metric function to evaluate the results.  \n", "> **NOTE:** do not forget all the preprocessing steps you had to do on the training set before using it as input to your clustering model. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b022: EVALUATE ON TEST DATA\n", "Whenever you feel that your model is ready, you should evaluate on unkwown test data\n", "\n", "You are provided with a 100-positions dataset, for which the n=100 closest Pok\u00e9mon\n", "were computed.\n", "\"\"\"\n", "\n", "import pickle\n", "\n", "X_test = pd.read_csv(\"Data/data-test-X.csv\")\n", "y_test = dict(\n", "    name=np.loadtxt(\"Data/data-test-y-name.txt\", dtype=str, delimiter=\",\"),\n", "    type=np.loadtxt(\"Data/data-test-y-type.txt\", dtype=str, delimiter=\",\"),\n", ")\n", "\n", "X_test\n", "\n", "#########################################################################################################\n", "# Start : Student version\n", "#########################################################################################################\n", "\n", "# evaluate your model(s) on the test data\n", "\n", "#########################################################################################################\n", "# End : Student version\n", "#########################################################################################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-warning\">\n", "<b>[Question 3.3] Your clustering solution </b> <br> \n", "Describe here your clustering solution (how many clusters, which method of sampling the Pok\u00e9mon, other important choices that have been made, etc.). Justify your choices with the help of the metric.\n", "</div>\n", "\n", "<div class=\"alert alert-info\">\n", "<b>[Remark 3.2] </b> <br>\n", "Do not hesitate to accompany your explanation with some graphs. You can support your answer by ploting, by example, the evolution of the quality of your predictions according to the number <code>k</code> of clusters.\n", "</div>\n", "\n", "<div class=\"alert alert-info\">\n", "<b>[Remark 3.3] </b> <br>\n", "    By default, the target output is set to the Pok\u00e9mon <code>name</code>. If you create a clustering model that predicts the name of a given Pok\u00e9mon, you can easily predict its type given a mapping between Pok\u00e9mon names and their type. This opposite is not true.<br>\n", "    \n", "However, this approach may not be the best if your only goal is to predict Pok\u00e9mon types, not names. This is why we encourage you to set the output to <code>type</code>, and compare the results between the two approaches. Question 3.4 is dedicated to this comparison.\n", "</div>\n", "\n", "<div class=\"alert alert-warning\">\n", "<b>[Question 3.4] Comparing models - BONUS</b> <br> \n", "Compare how your model performs when predicting the Pok\u00e9mon types, based on <code>output=\"type\"</code> versus <code>output=\"name\"</code>. I.e., does predicting Pok\u00e9mon types based on the Pok\u00e9mon names performs better than directly predicting the types?\n", "</div>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<font size=5 color=#009999>\n", "WELL DONE!\n", "</font>\n", "\n", "Now, you should be able to create a complete pipeline, from start to end, that can train a model on a data, and output some prediction for a given input vector. This will be seen during, e.g., the [LELEC2870 - Machine learning : regression, deep networks and dimensionality reduction](https://uclouvain.be/en-cours-2021-lelec2870) classes."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.6"}, "vscode": {"interpreter": {"hash": "6ba07c751260516eeb73d18fac358a7693501f65ad6727169b82411af5185fde"}}}, "nbformat": 4, "nbformat_minor": 2}